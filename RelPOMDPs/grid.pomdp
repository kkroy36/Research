//pomdp file description
stateVariables = [neartt,nearsc,nearwl,nearinfo]
states = [s1,s2,s3,s4,s5,s6,s7,s8]
initialBelief = [0.115384615385,0.153846153846,0.0576923076923,0.115384615385,0.192307692308,0.0769230769231,0.134615384615,0.153846153846]
stateFacts = [neartt(s1),neartt(s2),nearsc(s3),nearsc(s4),nearwl(s5),nearwl(s6),nearinfo(s7),nearinfo(s8)]
actions = [a1,a2,a3,a4]
observations = [sushibar,subway,lib,info]
observation: neartt(s1)=>sushibar(s1) 0.9
observation: neartt(s1)=>subway(s1) 0.1
observation: neartt(s2)=>sushibar(s2) 0.9
observation: neartt(s2)=>subway(s2) 0.1
observation: nearsc(s3)=>sushibar(s3) 0.1
observation: nearsc(s3)=>subway(s3) 0.9
observation: nearsc(s4)=>sushibar(s4) 0.1
observation: nearsc(s4)=>subway(s4) 0.9
observation: nearwl(s5)=>sushibar(s5) 0.1
observation: nearwl(s5)=>lib(s5) 0.9
observation: nearwl(s6)=>sushibar(s6) 0.1
observation: nearwl(s6)=>lib(s6) 0.9
observation: nearinfo(s7)=>subway(s7) 0.1
observation: nearinfo(s7)=>info(s7) 0.9
observation: nearinfo(s8)=>subway(s8) 0.1
observation: nearinfo(s8)=>info(s8) 0.9
//action a1
transition: T(s1,s1)^a1(s1) 0.5
transition: T(s1,s2)^a1(s1) 0.5
transition: T(s2,s1)^a1(s2) 0.5
transition: T(s2,s2)^a1(s2) 0.5
transition: T(s3,s1)^a1(s3) 0.5
transition: T(s3,s2)^a1(s3) 0.5
transition: T(s4,s1)^a1(s4) 0.5
transition: T(s4,s2)^a1(s4) 0.5
transition: T(s5,s5)^a1(s5) 0.5
transition: T(s5,s6)^a1(s5) 0.5
transition: T(s6,s5)^a1(s6) 0.5
transition: T(s6,s6)^a1(s6) 0.5
transition: T(s7,s5)^a1(s7) 0.5
transition: T(s7,s6)^a1(s7) 0.5
transition: T(s8,s5)^a1(s8) 0.5
transition: T(s8,s6)^a1(s8) 0.5
//action a2
transition: T(s1,s3)^a2(s1) 0.5
transition: T(s1,s4)^a2(s1) 0.5
transition: T(s2,s3)^a2(s2) 0.5
transition: T(s2,s4)^a2(s2) 0.5
transition: T(s3,s3)^a2(s3) 0.5
transition: T(s3,s4)^a2(s3) 0.5
transition: T(s4,s3)^a2(s4) 0.5
transition: T(s4,s4)^a2(s4) 0.5
transition: T(s5,s7)^a2(s5) 0.5
transition: T(s5,s8)^a2(s5) 0.5
transition: T(s6,s7)^a2(s6) 0.5
transition: T(s6,s8)^a2(s6) 0.5
transition: T(s7,s7)^a2(s7) 0.5
transition: T(s7,s8)^a2(s7) 0.5
transition: T(s8,s7)^a2(s8) 0.5
transition: T(s8,s8)^a2(s8) 0.5
//action a3
transition: T(s1,s1)^a3(s1) 0.5
transition: T(s1,s2)^a3(s1) 0.5
transition: T(s2,s1)^a3(s2) 0.5
transition: T(s2,s2)^a3(s2) 0.5
transition: T(s3,s3)^a3(s3) 0.5
transition: T(s3,s4)^a3(s3) 0.5
transition: T(s4,s3)^a3(s4) 0.5
transition: T(s4,s4)^a3(s4) 0.5
transition: T(s5,s1)^a3(s5) 0.5
transition: T(s5,s2)^a3(s5) 0.5
transition: T(s6,s1)^a3(s6) 0.5
transition: T(s6,s2)^a3(s6) 0.5
transition: T(s7,s3)^a3(s7) 0.5
transition: T(s7,s4)^a3(s7) 0.5
transition: T(s8,s3)^a3(s8) 0.5
transition: T(s8,s4)^a3(s8) 0.5
//action a4
transition: T(s1,s5)^a4(s1) 0.5
transition: T(s1,s6)^a4(s1) 0.5
transition: T(s2,s5)^a4(s2) 0.5
transition: T(s2,s6)^a4(s2) 0.5
transition: T(s3,s7)^a4(s3) 0.5
transition: T(s3,s8)^a4(s3) 0.5
transition: T(s4,s7)^a4(s4) 0.5
transition: T(s4,s8)^a4(s4) 0.5
transition: T(s5,s5)^a4(s5) 0.5
transition: T(s5,s6)^a4(s5) 0.5
transition: T(s6,s5)^a4(s6) 0.5
transition: T(s6,s6)^a4(s6) 0.5
transition: T(s7,s7)^a4(s7) 0.5
transition: T(s7,s8)^a4(s7) 0.5
transition: T(s8,s7)^a4(s8) 0.5
transition: T(s8,s8)^a4(s8) 0.5
//actions done..
reward: r(s1,-1)
reward: r(s2,-1)
reward: r(s3,-1)
reward: r(s4,-1)
reward: r(s5,-1)
reward: r(s6,-1)
reward: r(s7,-1)
reward: r(s8,50)
horizon = 1
discountFactor = 0.9
